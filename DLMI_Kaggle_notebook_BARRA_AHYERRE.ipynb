{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Challenge DLMI\n",
    "\n",
    "This notebook is our model for the data challenge of out of distribution classification of histopathology patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:11:56.799972Z",
     "iopub.status.busy": "2025-04-08T16:11:56.799646Z",
     "iopub.status.idle": "2025-04-08T16:12:04.056017Z",
     "shell.execute_reply": "2025-04-08T16:12:04.055070Z",
     "shell.execute_reply.started": "2025-04-08T16:11:56.799944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torchvision.models as models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:12:04.057895Z",
     "iopub.status.busy": "2025-04-08T16:12:04.057219Z",
     "iopub.status.idle": "2025-04-08T16:12:04.062039Z",
     "shell.execute_reply": "2025-04-08T16:12:04.060935Z",
     "shell.execute_reply.started": "2025-04-08T16:12:04.057858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data files: training, validation and test datasest\n",
    "TRAIN_IMAGES_PATH = 'train.h5'\n",
    "VAL_IMAGES_PATH = 'val.h5'\n",
    "TEST_IMAGES_PATH = 'test.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:12:04.407869Z",
     "iopub.status.busy": "2025-04-08T16:12:04.407489Z",
     "iopub.status.idle": "2025-04-08T16:12:04.458867Z",
     "shell.execute_reply": "2025-04-08T16:12:04.457875Z",
     "shell.execute_reply.started": "2025-04-08T16:12:04.407831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu.\n"
     ]
    }
   ],
   "source": [
    "# We worked mostly on cpu (local machine) but much faster on gpu (under Kaggle gpu credits)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Working on {device}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure full **reproducibility** of the experiments, we set the random seed across all libraries involved in randomness: `numpy`, `random`, `torch` and the Python environment. We also explicitly configure PyTorch for deterministic behavior and use a seeded generator for `DataLoader` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:12:06.215494Z",
     "iopub.status.busy": "2025-04-08T16:12:06.215198Z",
     "iopub.status.idle": "2025-04-08T16:12:06.228154Z",
     "shell.execute_reply": "2025-04-08T16:12:06.227176Z",
     "shell.execute_reply.started": "2025-04-08T16:12:06.215473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x78c47bc5b070>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0 # seed for reproducibility\n",
    "\n",
    "# set the seed for all libraries\n",
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# additional seeds for PyTorch and CUDA since the final training was done on Kaggle GPU (multi GPU compatible)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True # enforce deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.benchmark = False # disable benchmark optimizations that are not deterministic\n",
    "\n",
    "# set Python hash seed \n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# create a torch.Generator for deterministic DataLoader shuffling\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to the data\n",
    "As explained in the `getting_started` baseline notebook, the dataset consists of patches of whole slide images which should be classified into either containing tumor or not. The training images come from 3 different centers (i.e. hospitals), while the validation set comes from another center and the test set from yet another center. The visual aspect of the patches are quite different due to the slightly different staining procedures, conditions, and equipment from each hospital. \n",
    "\n",
    "The data is stored in `.h5` files, which can be seen as a folder hierarchy, which are can be seen as the following.\n",
    "```\n",
    "├── idx           # index of the image\n",
    "│   └── img       # image in a tensor format\n",
    "│   └── label     # binary label of the image\n",
    "│   └── metadata  # some metadata on the images\n",
    "```\n",
    "\n",
    "The following is a visualization of how different the images look from the different centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:12:08.102895Z",
     "iopub.status.busy": "2025-04-08T16:12:08.102513Z",
     "iopub.status.idle": "2025-04-08T16:12:08.107911Z",
     "shell.execute_reply": "2025-04-08T16:12:08.106702Z",
     "shell.execute_reply.started": "2025-04-08T16:12:08.102866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# initialize training and validation dictionaries to hold one sample per (center, label) combination\n",
    "train_images = {0: {0: None, 1: None},\n",
    "                3: {0: None, 1: None},\n",
    "                4: {0: None, 1: None}}\n",
    "val_images = {1: {0: None, 1: None}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:13:39.030110Z",
     "iopub.status.busy": "2025-04-08T16:13:39.029883Z",
     "iopub.status.idle": "2025-04-08T16:14:43.157409Z",
     "shell.execute_reply": "2025-04-08T16:14:43.156385Z",
     "shell.execute_reply.started": "2025-04-08T16:13:39.030091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Same code as in the getting_started notebook to load the data\n",
    "# we loop over training and validation paths to extract the samples\n",
    "for img_data, data_path in zip([train_images, val_images], [TRAIN_IMAGES_PATH, VAL_IMAGES_PATH]):\n",
    "    with h5py.File(data_path, 'r') as hdf:\n",
    "        for img_idx in list(hdf.keys()):\n",
    "            label = int(np.array(hdf.get(img_idx).get('label'))) # read label 0 or 1\n",
    "            center = int(np.array(hdf.get(img_idx).get('metadata'))[0]) # read center\n",
    "\n",
    "            # if this (center, label) slot is not filled, store the image\n",
    "            if img_data[center][label] is None:\n",
    "                img_data[center][label] = np.array(hdf.get(img_idx).get('img'))\n",
    "\n",
    "            # break early if all required combinations are filled\n",
    "            if all(all(value is not None for value in inner_dict.values()) for inner_dict in img_data.values()):\n",
    "                break\n",
    "\n",
    "# merge training and validation dictionaries\n",
    "all_data = {**train_images, **val_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualizes one image per (center, label) combination\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "center_ids = {center: idx for idx, center in enumerate(all_data.keys())}\n",
    "for center in all_data:\n",
    "    for label in all_data[center]:\n",
    "        axs[label, center_ids[center]].imshow(np.moveaxis(all_data[center][label], 0, -1).astype(np.float32))\n",
    "        axs[label, center_ids[center]].axis('off')\n",
    "        if label == 0:\n",
    "            axs[label, center_ids[center]].set_title(f'Center {center}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline DINOv2 model and LoRA fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:16:10.120661Z",
     "iopub.status.busy": "2025-04-08T16:16:10.120327Z",
     "iopub.status.idle": "2025-04-08T16:16:10.124451Z",
     "shell.execute_reply": "2025-04-08T16:16:10.123451Z",
     "shell.execute_reply.started": "2025-04-08T16:16:10.120633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines two custom PyTorch datasets:\n",
    "We use the two custom datasets definition function given in the `getting_started` notebook to use DINOv2.\n",
    "\n",
    "- `BaselineDataset` loads raw image data from HDF5 files for training or for inference\n",
    "- `PrecomputedDataset` loads pre-extracted features (e.g. from DINOv2) and their corresponding labels for training a small classifier (e.g. linear probing).\n",
    "\n",
    "These dataset classes will be used in feature extraction and classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:14:43.159728Z",
     "iopub.status.busy": "2025-04-08T16:14:43.159370Z",
     "iopub.status.idle": "2025-04-08T16:14:43.165631Z",
     "shell.execute_reply": "2025-04-08T16:14:43.164622Z",
     "shell.execute_reply.started": "2025-04-08T16:14:43.159689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset class for loading raw images and labels from HDF5 files\n",
    "class BaselineDataset(Dataset):\n",
    "    def __init__(self, dataset_path, preprocessing, mode):\n",
    "        super(BaselineDataset, self).__init__()\n",
    "        self.dataset_path = dataset_path # path to HDF5 file\n",
    "        self.preprocessing = preprocessing # transformations to apply to the image (resizing in our case)\n",
    "        self.mode = mode # 'train' or other ('test' for inferance for instance)\n",
    "\n",
    "        # load image ID from the HDF5 file\n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:        \n",
    "            self.image_ids = list(hdf.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:\n",
    "            img = torch.tensor(hdf.get(img_id).get('img')).float() # load the image\n",
    "            label = np.array(hdf.get(img_id).get('label')) if self.mode == 'train' else None # label the image\n",
    "        return self.preprocessing(img).float(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:14:43.167008Z",
     "iopub.status.busy": "2025-04-08T16:14:43.166735Z",
     "iopub.status.idle": "2025-04-08T16:14:43.180112Z",
     "shell.execute_reply": "2025-04-08T16:14:43.179151Z",
     "shell.execute_reply.started": "2025-04-08T16:14:43.166975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset class for loading precomputed features and corresponding labels\n",
    "class PrecomputedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super(PrecomputedDataset, self).__init__()\n",
    "        self.features = features # precomputed image features\n",
    "        self.labels = labels.unsqueeze(-1) # add dimension for binary classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the pretrained feature extractor from DINOv2. DINOv2 is a self supervised learning model for visual representation learning. It uses vision transformers (ViTs) to learn representative features without requiring a large amount of labeled data. Bellow we lead the small model DINOv2 ViT-S/14 since it will be easier to fine tune. Also DINOv2 is known to be transferable and performant for downstream tasks such as image classification, which is what we aim to do [[1]](https://arxiv.org/abs/2304.07193)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:14:43.181379Z",
     "iopub.status.busy": "2025-04-08T16:14:43.181056Z",
     "iopub.status.idle": "2025-04-08T16:14:44.418730Z",
     "shell.execute_reply": "2025-04-08T16:14:44.417816Z",
     "shell.execute_reply.started": "2025-04-08T16:14:43.181349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we are fine tuning DINOv2 using **Low Rank Adpatation** (**LoRA**) and attach a small classifier head for binary classification.\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning method that inserts low rank adapters into specific layers of a large model. Instead of updating all weights of the DINOv2 backbone, we only train a small number of parameters, which reduces memory and computational costs. It is indeed relevant in our case where we worked mostly on CPU and with limited access to GPU.\n",
    "\n",
    "We apply LoRA to the query/key/value (QKV) attention blocks of all 12 transformer layers in the ViT-S. A simple two-layer feedforward classifier (with 64 hidden units and a ReLU activation) is stacked on top of the frozen DINOv2 features for binary classification.\n",
    "\n",
    "We use the AdamW optimizer and the binary cross-entropy loss (BCE) since our task is binary classification (cancer detection or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:14:44.419851Z",
     "iopub.status.busy": "2025-04-08T16:14:44.419561Z",
     "iopub.status.idle": "2025-04-08T16:14:44.448466Z",
     "shell.execute_reply": "2025-04-08T16:14:44.447811Z",
     "shell.execute_reply.started": "2025-04-08T16:14:44.419828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 73,728 || all params: 22,130,304 || trainable%: 0.3332\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to specific attention layers in the DINOv2 transformer\n",
    "lora_config = LoraConfig(\n",
    "    r=4, # rank of the low rank decomposition. Chosen among [2, 4, 8, 16] to give the best test accuracy\n",
    "    lora_alpha=16, # scaling factor of the adapter weights. Chosen among [8, 16, 32]\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[f\"blocks.{i}.attn.qkv\" for i in range(12)] # apply LoRA to all attention QKV layers\n",
    ")\n",
    "\n",
    "# merge the DINOv2 model with LoRA so that only the LoRA parameters will be trained\n",
    "feature_extractor = get_peft_model(feature_extractor, lora_config)\n",
    "feature_extractor.print_trainable_parameters() # print the number of trainable parameters\n",
    "\n",
    "# set the model to training mode (only LoRA params will be trained)\n",
    "feature_extractor.train()\n",
    "\n",
    "# binary classifier on top of the DINOv2 features (CLS token of dim 384)\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(384, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid() # convert logits to probabilities (for BCE loss)\n",
    ").to(device)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(feature_extractor.parameters()) + list(classifier.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Binary Cross Entropy loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the training and validation datasets from HDF5 files using the `BaselineDataset` class. Images are resized to 98×98, which matches the resolution used by DINOv2. Then, we prepare a `DataLoader` for each set.\n",
    "\n",
    "Due to the high computational cost of fine tuning large models (even with LoRA), we perform LoRA training on a $10$% random subset of the training data. This allows us to speed up training (and actually it gave us a slightly better test accuracy than when training with the full train dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:16:18.888931Z",
     "iopub.status.busy": "2025-04-08T16:16:18.888550Z",
     "iopub.status.idle": "2025-04-08T16:17:22.830365Z",
     "shell.execute_reply": "2025-04-08T16:17:22.829630Z",
     "shell.execute_reply.started": "2025-04-08T16:16:18.888904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# resizing transformation for both training and validation sets (98x98 for DINOv2)\n",
    "preprocessing = transforms.Resize((98, 98))\n",
    "\n",
    "# load the full training and validation datasets from HDF5 files\n",
    "train_dataset = BaselineDataset(TRAIN_IMAGES_PATH, preprocessing, 'train')\n",
    "val_dataset = BaselineDataset(VAL_IMAGES_PATH, preprocessing, 'train')\n",
    "\n",
    "# we create DataLoaders for the full training and validation sets\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, generator=g, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, generator=g)\n",
    "\n",
    "# then we use 10% of train set for LoRA fine-tuning\n",
    "subset_size = int(0.1 * len(train_dataset))\n",
    "subset_indices = torch.randperm(len(train_dataset), generator=g)[:subset_size]\n",
    "lora_subset = Subset(train_dataset, subset_indices)\n",
    "lora_loader = DataLoader(lora_subset, shuffle=True, batch_size=BATCH_SIZE, generator=g, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop for fine tuning DINOv2-LoRA model. We save training and validation loss and accuracy at each epoch, and save the model that decrease the validation loss the most.\n",
    "\n",
    "We perform \"only\" $5$ epochs first because training is quite long to perform (especially the validation phase where more than $2000$ images need to be tested), but also because the training loss reduces drastically after 1 epochs only and does not descrease much then, while the validation loss and accuracy don't improve much with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# initialize best validation loss for early model saving\n",
    "best_val_loss = float(\"inf\")\n",
    "MODEL_SAVE_PATH = \"dinov2_lora_trained.pth\"\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    # training loop\n",
    "    classifier.train()\n",
    "    feature_extractor.train()\n",
    "    train_losses, train_accuracies = [], []\n",
    "    for images, labels in tqdm(lora_loader, desc=f\"Epoch {epoch+1} - LoRA Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # forward pass\n",
    "        features = feature_extractor(images)\n",
    "        if features.dim() == 3:\n",
    "            features = features[:, 0, :] # extract CLS token\n",
    "\n",
    "        preds = classifier(features).squeeze(-1)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # store batch loss and accuracy\n",
    "        train_losses.append(loss.item())\n",
    "        acc = ((preds > 0.5).float() == labels).float().mean().item()\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "    # average training metrics\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_train_acc = np.mean(train_accuracies)\n",
    "\n",
    "    # validation loop\n",
    "    classifier.eval()\n",
    "    feature_extractor.eval()\n",
    "    val_losses, val_accuracies = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # forward pass \n",
    "            features = feature_extractor(images)\n",
    "            if features.dim() == 3:\n",
    "                features = features[:, 0, :]\n",
    "\n",
    "            preds = classifier(features).squeeze(-1)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            acc = ((preds > 0.5).float() == labels).float().mean().item()\n",
    "            val_accuracies.append(acc)\n",
    "\n",
    "    # average validation metrics\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    avg_val_acc = np.mean(val_accuracies)\n",
    "\n",
    "    # print the training and validation loss and accuracy for this epoch\n",
    "    print(f\"[LoRA Training] Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
    "\n",
    "    # save model if it improves on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(f'New best loss {best_val_loss:.4f} -> {avg_val_loss:.4f}')\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            \"feature_extractor\": feature_extractor.state_dict(),\n",
    "            \"classifier\": classifier.state_dict()\n",
    "        }, MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fine tuning the DINOv2 backbone using LoRA and training a lightweight classifier on top, we now reload the best saved checkpoint to reuse the trained models for evaluation or inference. This ensures reproducibility and avoids retraining the entire model.\n",
    "\n",
    "After fine tuning the DINOv2+LoRA model, we load the best saved checkpoint (since the last feature_extractor obtained is not necessarily the one we are going to use for inference). We define the DINOv2+LoRA model as before but with loaded `feature_extractor` and `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:19:17.332136Z",
     "iopub.status.busy": "2025-04-08T16:19:17.331751Z",
     "iopub.status.idle": "2025-04-08T16:19:18.523479Z",
     "shell.execute_reply": "2025-04-08T16:19:18.522698Z",
     "shell.execute_reply.started": "2025-04-08T16:19:17.332110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# moad saved model checkpoint \n",
    "checkpoint = torch.load(\"dinov2_lora_trained.pth\", map_location=\"cpu\")\n",
    "\n",
    "# reinitialize the DINOv2 feature extractor\n",
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "# redefine the classifier architecture that was trained on top of DINOv2 features\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(384, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "# apply LoRA to the DINOv2 model (same configuration as during training)\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[f\"blocks.{i}.attn.qkv\" for i in range(12)],\n",
    ")\n",
    "feature_extractor = get_peft_model(feature_extractor, lora_config)\n",
    "\n",
    "# load the fine tuned weights into the feature extractor and classifier\n",
    "feature_extractor.load_state_dict(checkpoint[\"feature_extractor\"])\n",
    "classifier.load_state_dict(checkpoint[\"classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a linear probing on top of the DINOv2+LoRA model\n",
    "\n",
    "To speed up the training of the final classifiers, we precompute the features extracted by the DINOv2+LoRA model. The following function `precompute_features_lora` extracts features for all images in a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-08T16:15:46.075172Z",
     "iopub.status.idle": "2025-04-08T16:15:46.075535Z",
     "shell.execute_reply": "2025-04-08T16:15:46.075362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def precompute_features_lora(dataloader, model, device):\n",
    "    xs, ys = [], [] # will store extracted features xs and labels ys\n",
    "    model.eval() # set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dataloader):\n",
    "            feats = model(x.to(device)) # the model extracts features from the image\n",
    "\n",
    "            # if the output has a CLS token (since we work with a ViT), then keep only the CLS embedding\n",
    "            if feats.dim() == 3:\n",
    "                feats = feats[:, 0, :]\n",
    "\n",
    "            xs.append(feats.cpu()) # move features to cpu\n",
    "            ys.append(torch.tensor(y)) # store the label\n",
    "\n",
    "    # concatenate all batches into a single tensor of features and labels\n",
    "    return torch.cat(xs), torch.cat(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To efficiently train the linear probing classifier on top of the frozen DINOv2 + LoRA model, we first precompute the features from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the training dataset with class BaselineDataset (preprocessing = resizing)\n",
    "raw_train_dataset = BaselineDataset(TRAIN_IMAGES_PATH, preprocessing, 'train')\n",
    "# create a DataLoader for the training dataset\n",
    "raw_train_dataloader = DataLoader(raw_train_dataset, shuffle=True, batch_size=BATCH_SIZE, generator=g, drop_last=True)\n",
    "# extract features using the pretrained DINOv2+LoRA model\n",
    "train_features, train_labels = precompute_features_lora(raw_train_dataloader, feature_extractor, device)\n",
    "torch.save((train_features, train_labels), 'train_features_dinov2_lora.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same with the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the validation dataset with class BaselineDataset (preprocessing = resizing)\n",
    "raw_val_dataset = BaselineDataset(VAL_IMAGES_PATH, preprocessing, 'train')\n",
    "# create a DataLoader for the validation dataset\n",
    "raw_val_dataloader = DataLoader(raw_val_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=g, drop_last=True)\n",
    "# extract features using the pretrained DINOv2+LoRA model\n",
    "val_features, val_labels = precompute_features_lora(raw_val_dataloader, feature_extractor, device)\n",
    "torch.save((val_features, val_labels), 'val_features_dinov2_lora.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After precomputation of the DINOv2+LoRA features, we can load them to train the linear probing classifier. We use the `PrecomputedDataset` class to create the training and validation datasets (with precomputed features) and create as before a training and validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:19:36.737000Z",
     "iopub.status.busy": "2025-04-08T16:19:36.736672Z",
     "iopub.status.idle": "2025-04-08T16:19:36.857083Z",
     "shell.execute_reply": "2025-04-08T16:19:36.856168Z",
     "shell.execute_reply.started": "2025-04-08T16:19:36.736978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load precomputed feature tensors and labels from disk for both train and validation sets\n",
    "train_features, train_labels = torch.load('train_features_dinov2_lora.pth')\n",
    "val_features, val_labels = torch.load('val_features_dinov2_lora.pth')\n",
    "\n",
    "# wrap the tensors into a custom Dataset class for PyTorch compatibility\n",
    "train_dataset = PrecomputedDataset(train_features, train_labels)\n",
    "val_dataset = PrecomputedDataset(val_features, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, generator=g, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, generator=g, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a simple classifier head `linear_probing`, on top of the precomputed DINOv2+LoRA features. The classifier consists of two fully connected layers with a ReLU activation (for non linearity), followed by a sigmoid for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:19:39.731469Z",
     "iopub.status.busy": "2025-04-08T16:19:39.731167Z",
     "iopub.status.idle": "2025-04-08T16:19:39.737323Z",
     "shell.execute_reply": "2025-04-08T16:19:39.736479Z",
     "shell.execute_reply.started": "2025-04-08T16:19:39.731448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# linear probing classifier (also referred to as linear probing)\n",
    "# Input: 384-dimensional features from DINOv2\n",
    "# Hidden dimension: 128 (was tuned among [64, 128, 256])\n",
    "# Output: single neuron with sigmoid for binary classification\n",
    "linear_probing = torch.nn.Sequential(\n",
    "    torch.nn.Linear(384, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kept almost the same training loop for training the classification head as in the `getting_started` notebook, using Adam optimizer and BCE loss (known to be performant for binary classification especially when the classes are balanced). We apply early stopping based on the validtion loss as the model that achieves the lowest validation loss is saved. In our fine tuning experiments, we obersved that in most of the cases, the best model was achieved during the first epochs (never we have trained until $100$ epochs), indicating that the model overfit the training set if we don't do early stopping.\n",
    "\n",
    "Note: we did not experiment fine tuning the learning rate neither the weight decay, which could also lead to improvement of the final prediction if carefully chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:19:47.973152Z",
     "iopub.status.busy": "2025-04-08T16:19:47.972845Z",
     "iopub.status.idle": "2025-04-08T16:19:47.977110Z",
     "shell.execute_reply": "2025-04-08T16:19:47.976220Z",
     "shell.execute_reply.started": "2025-04-08T16:19:47.973131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "OPTIMIZER = 'Adam'\n",
    "OPTIMIZER_PARAMS = {'lr': 0.001, 'weight_decay': 0.005}\n",
    "LOSS = 'BCELoss'\n",
    "METRIC = 'Accuracy'\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:19:55.302219Z",
     "iopub.status.busy": "2025-04-08T16:19:55.301889Z",
     "iopub.status.idle": "2025-04-08T16:25:48.149518Z",
     "shell.execute_reply": "2025-04-08T16:25:48.148513Z",
     "shell.execute_reply.started": "2025-04-08T16:19:55.302194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [1/30] | Loss 0.0745 | Metric 0.9742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [1/30] | Loss 0.1996 | Metric 0.9237\n",
      "New best loss inf -> 0.1996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [2/30] | Loss 0.0708 | Metric 0.9757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [2/30] | Loss 0.2121 | Metric 0.9163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [3/30] | Loss 0.0695 | Metric 0.9763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [3/30] | Loss 0.2057 | Metric 0.9190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [4/30] | Loss 0.0692 | Metric 0.9766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [4/30] | Loss 0.2165 | Metric 0.9178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [5/30] | Loss 0.0687 | Metric 0.9765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [5/30] | Loss 0.1874 | Metric 0.9286\n",
      "New best loss 0.1996 -> 0.1874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [6/30] | Loss 0.0690 | Metric 0.9762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [6/30] | Loss 0.2026 | Metric 0.9184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [7/30] | Loss 0.0687 | Metric 0.9765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [7/30] | Loss 0.2197 | Metric 0.9238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [8/30] | Loss 0.0687 | Metric 0.9765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [8/30] | Loss 0.1950 | Metric 0.9251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [9/30] | Loss 0.0689 | Metric 0.9764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [9/30] | Loss 0.1876 | Metric 0.9263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [10/30] | Loss 0.0687 | Metric 0.9763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [10/30] | Loss 0.2047 | Metric 0.9206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [11/30] | Loss 0.0690 | Metric 0.9758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [11/30] | Loss 0.1948 | Metric 0.9257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [12/30] | Loss 0.0686 | Metric 0.9762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [12/30] | Loss 0.1965 | Metric 0.9258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [13/30] | Loss 0.0689 | Metric 0.9762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [13/30] | Loss 0.2165 | Metric 0.9153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [14/30] | Loss 0.0690 | Metric 0.9762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [14/30] | Loss 0.2569 | Metric 0.9070\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [15/30] | Loss 0.0689 | Metric 0.9761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [15/30] | Loss 0.1991 | Metric 0.9236\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = getattr(torch.optim, OPTIMIZER)(linear_probing.parameters(), **OPTIMIZER_PARAMS)\n",
    "criterion = getattr(torch.nn, LOSS)()\n",
    "\n",
    "# the used metric is the accuracy\n",
    "train_metric_fn = BinaryAccuracy().to(device)\n",
    "val_metric_fn = BinaryAccuracy().to(device)\n",
    "\n",
    "# early stopping\n",
    "min_loss, best_epoch = float('inf'), 0\n",
    "\n",
    "# set classifier to training mode\n",
    "linear_probing.train()\n",
    "for param in linear_probing.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    linear_probing.train()\n",
    "    train_metrics, train_losses = [], []\n",
    "\n",
    "    for train_x, train_y in tqdm(train_dataloader, leave=False):\n",
    "        train_y = train_y.squeeze(-1)\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = linear_probing(train_x).squeeze(-1)\n",
    "        loss = criterion(train_pred, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.extend([loss.item()] * len(train_y))\n",
    "        train_metric = train_metric_fn(train_pred, train_y.int())\n",
    "        train_metrics.extend([train_metric.item()] * len(train_y))\n",
    "\n",
    "    print(f'Epoch train [{epoch+1}/{NUM_EPOCHS}] | Loss {np.mean(train_losses):.4f} | Metric {np.mean(train_metrics):.4f}')\n",
    "\n",
    "    # validation\n",
    "    linear_probing.eval()\n",
    "    val_metrics, val_losses = [], []\n",
    "\n",
    "    for val_x, val_y in tqdm(val_dataloader, leave=False):\n",
    "        val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "        val_y = val_y.squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pred = linear_probing(val_x).squeeze(-1)\n",
    "        loss = criterion(val_pred, val_y)\n",
    "        val_losses.extend([loss.item()] * len(val_y))\n",
    "        val_metric = val_metric_fn(val_pred, val_y.int())\n",
    "        val_metrics.extend([val_metric.item()] * len(val_y))\n",
    "\n",
    "    print(f'Epoch valid [{epoch+1}/{NUM_EPOCHS}] | Loss {np.mean(val_losses):.4f} | Metric {np.mean(val_metrics):.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if np.mean(val_losses) < min_loss:\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        print(f'New best loss {min_loss:.4f} -> {mean_val_loss:.4f}')\n",
    "        min_loss = mean_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(linear_probing.state_dict(), 'best_model_lora.pth')\n",
    "\n",
    "    if epoch - best_epoch >= PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:30:16.930682Z",
     "iopub.status.busy": "2025-04-08T16:30:16.930305Z",
     "iopub.status.idle": "2025-04-08T16:30:16.940472Z",
     "shell.execute_reply": "2025-04-08T16:30:16.939687Z",
     "shell.execute_reply.started": "2025-04-08T16:30:16.930653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_probing.load_state_dict(torch.load('best_model_lora.pth', weights_only=True))\n",
    "linear_probing.eval()\n",
    "linear_probing.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a classification head on top of the pretrained ResNet34\n",
    "\n",
    "We also worked with several other models that are composed of CNNs, such as well known ResNets model. Especially, `resnet34` provided us the best test accuracy when ensembled with the DINOv2+LoRA model. Bellow is the code for the training of a classification head on top the the pretrained model.\n",
    "\n",
    "First, we prepare the data loaders as for the DINOv2 model, but with a different `resnet_preprocessing`, resizing images to 224x224 since it is the expected dimension by ResNet. As before, we train with only $10$% of the training dataset, but also used $30$% of the validation dataset to accelerate the validation phase during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:30:24.953852Z",
     "iopub.status.busy": "2025-04-08T16:30:24.953511Z",
     "iopub.status.idle": "2025-04-08T16:31:23.408148Z",
     "shell.execute_reply": "2025-04-08T16:31:23.407169Z",
     "shell.execute_reply.started": "2025-04-08T16:30:24.953826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# new preprocessing : resizing to 224x224\n",
    "resnet_preprocessing = transforms.Resize((224, 224))\n",
    "\n",
    "# load training and validation datasets\n",
    "raw_train_dataset_resnet = BaselineDataset(TRAIN_IMAGES_PATH, resnet_preprocessing, 'train')\n",
    "raw_val_dataset_resnet = BaselineDataset(VAL_IMAGES_PATH, resnet_preprocessing, 'train')\n",
    "\n",
    "# use 10% of the training dataset\n",
    "train_total_len = len(raw_train_dataset_resnet)\n",
    "train_subset_len = int(0.1 * train_total_len)\n",
    "_ , train_subset_resnet = random_split(raw_train_dataset_resnet, [train_total_len - train_subset_len, train_subset_len], generator=g)\n",
    "\n",
    "# use 30% of the validation dataset\n",
    "val_total_len = len(raw_val_dataset_resnet)\n",
    "val_subset_len = int(0.3 * val_total_len)\n",
    "val_subset_resnet, _ = random_split(raw_val_dataset_resnet, [val_subset_len, val_total_len - val_subset_len], generator=g)\n",
    "\n",
    "# training and validation dataloaders\n",
    "train_dataloader_resnet = DataLoader(train_subset_resnet, shuffle=True, batch_size=BATCH_SIZE, generator=g, drop_last=True)\n",
    "val_dataloader_resnet = DataLoader(val_subset_resnet, shuffle=False, batch_size=BATCH_SIZE, generator=g, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we load the pretrained resnet34 and replace the last fully connected layer for a 2 layer MLP as a classification head. \n",
    "\n",
    "We also use Adam and BCE loss for the training of the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:31:24.475732Z",
     "iopub.status.busy": "2025-04-08T16:31:24.475442Z",
     "iopub.status.idle": "2025-04-08T16:31:24.921368Z",
     "shell.execute_reply": "2025-04-08T16:31:24.920665Z",
     "shell.execute_reply.started": "2025-04-08T16:31:24.475709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resnet_model = models.resnet34(pretrained=True)\n",
    "in_features = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "resnet_optimizer = torch.optim.AdamW(resnet_model.parameters(), lr=1e-4)\n",
    "resnet_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T16:36:46.645237Z",
     "iopub.status.busy": "2025-04-08T16:36:46.644886Z",
     "iopub.status.idle": "2025-04-08T18:40:12.164981Z",
     "shell.execute_reply": "2025-04-08T18:40:12.163908Z",
     "shell.execute_reply.started": "2025-04-08T16:36:46.645212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 1:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b729bde669a9>:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  img = torch.tensor(hdf.get(img_id).get('img')).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 1: Loss 0.1490, Acc 0.9481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 1:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 1: Loss 0.2502, Acc 0.8984\n",
      "New best ResNet loss: inf -> 0.2502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 2:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 2: Loss 0.0820, Acc 0.9726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 2:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 2: Loss 0.2452, Acc 0.9100\n",
      "New best ResNet loss: 0.2502 -> 0.2452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 3:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 3: Loss 0.0572, Acc 0.9811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 3:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 3: Loss 0.2457, Acc 0.9156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 4:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 4: Loss 0.0367, Acc 0.9867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 4:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 4: Loss 0.3059, Acc 0.9106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 5:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 5: Loss 0.0297, Acc 0.9897\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 5:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 5: Loss 0.3384, Acc 0.8874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 6:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 6: Loss 0.0211, Acc 0.9925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 6:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 6: Loss 0.2848, Acc 0.9248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 7:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 7: Loss 0.0254, Acc 0.9912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 7:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 7: Loss 0.3408, Acc 0.9046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 8:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 8: Loss 0.0199, Acc 0.9939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 8:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 8: Loss 0.2173, Acc 0.9370\n",
      "New best ResNet loss: 0.2452 -> 0.2173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 9:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 9: Loss 0.0187, Acc 0.9949\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 9:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 9: Loss 0.7923, Acc 0.8088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Epoch 10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Train Epoch 10: Loss 0.0141, Acc 0.9946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ResNet Val Epoch 10:   0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Val Epoch 10: Loss 0.3592, Acc 0.9081\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS_RESNET = 10\n",
    "min_loss_resnet, best_epoch_resnet = float('inf'), 0\n",
    "\n",
    "# training loop for resnet34\n",
    "for epoch in range(NUM_EPOCHS_RESNET):\n",
    "    resnet_model.train()\n",
    "    train_losses_resnet, train_metrics_resnet = [], []\n",
    "\n",
    "    for images, labels in tqdm(train_dataloader_resnet, leave=False, desc=f'ResNet Epoch {epoch+1}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        resnet_optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        preds = resnet_model(images).squeeze(-1)\n",
    "        loss = resnet_criterion(preds, labels.float())\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        resnet_optimizer.step()\n",
    "\n",
    "        # training loss and accuracy\n",
    "        train_losses_resnet.append(loss.item())\n",
    "        preds_binary = (preds > 0.5).float()\n",
    "        acc = (preds_binary == labels.float()).float().mean().item()\n",
    "        train_metrics_resnet.append(acc)\n",
    "\n",
    "    print(f'ResNet Train Epoch {epoch+1}: Loss {np.mean(train_losses_resnet):.4f}, Acc {np.mean(train_metrics_resnet):.4f}')\n",
    "\n",
    "    # validation\n",
    "    resnet_model.eval()\n",
    "    val_losses_resnet, val_metrics_resnet = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader_resnet, leave=False, desc=f'ResNet Val Epoch {epoch+1}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            preds = resnet_model(images).squeeze(-1)\n",
    "            loss = resnet_criterion(preds, labels.float())\n",
    "\n",
    "            # validation loss and accuracy\n",
    "            val_losses_resnet.append(loss.item())\n",
    "            preds_binary = (preds > 0.5).float()\n",
    "            acc = (preds_binary == labels.float()).float().mean().item()\n",
    "            val_metrics_resnet.append(acc)\n",
    "\n",
    "    print(f'ResNet Val Epoch {epoch+1}: Loss {np.mean(val_losses_resnet):.4f}, Acc {np.mean(val_metrics_resnet):.4f}')\n",
    "\n",
    "    # save the best model that minimize the validation loss\n",
    "    if np.mean(val_losses_resnet) < min_loss_resnet:\n",
    "        print(f'New best ResNet loss: {min_loss_resnet:.4f} -> {np.mean(val_losses_resnet):.4f}')\n",
    "        min_loss_resnet = np.mean(val_losses_resnet)\n",
    "        best_epoch_resnet = epoch\n",
    "        torch.save(resnet_model.state_dict(), 'best_resnet34.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the best ResNet-34 model + trained classification head checkpoint (based on validation loss) and switch it to evaluation mode to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:46:29.504167Z",
     "iopub.status.busy": "2025-04-08T18:46:29.503847Z",
     "iopub.status.idle": "2025-04-08T18:46:29.590942Z",
     "shell.execute_reply": "2025-04-08T18:46:29.590020Z",
     "shell.execute_reply.started": "2025-04-08T18:46:29.504141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-fd71791f05a1>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet_model.load_state_dict(torch.load('/kaggle/working/best_resnet34.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.load_state_dict(torch.load('best_resnet34.pth'))\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Prediction\n",
    "\n",
    "We now generate predictions on the test set using three configurations:\n",
    "\n",
    "- DINOv2+LoRA + linear probing\n",
    "- ResNet34 + trained classification head\n",
    "- Ensemble: the average prediction of the two models\n",
    "\n",
    "For each, we save both binary predictions and raw scores to CSV files, which can be used later for evaluation, ensemble analysis or correlation heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:46:39.386292Z",
     "iopub.status.busy": "2025-04-08T18:46:39.385980Z",
     "iopub.status.idle": "2025-04-08T18:47:43.461420Z",
     "shell.execute_reply": "2025-04-08T18:47:43.460758Z",
     "shell.execute_reply.started": "2025-04-08T18:46:39.386269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
    "    test_ids = list(hdf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T18:48:08.939908Z",
     "iopub.status.busy": "2025-04-08T18:48:08.939536Z",
     "iopub.status.idle": "2025-04-08T19:18:20.405204Z",
     "shell.execute_reply": "2025-04-08T19:18:20.404004Z",
     "shell.execute_reply.started": "2025-04-08T18:48:08.939881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# initialize dictionaries to store predictions\n",
    "solutions_dinov2_lora = {'ID': [], 'Pred': []}\n",
    "solutions_dinov2_lora_raw = {'ID': [], 'Raw': []}\n",
    "solutions_resnet34 = {'ID': [], 'Pred': []}\n",
    "solutions_resnet34_raw = {'ID': [], 'Raw': []}\n",
    "solutions_dinov2_lora_resnet34 = {'ID': [], 'Pred': []}\n",
    "\n",
    "# inference loop\n",
    "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
    "    for test_id in tqdm(test_ids):\n",
    "        # DINOv2+LoRA prediction\n",
    "        img_dinov2_lora = preprocessing(torch.tensor(np.array(hdf.get(test_id).get('img')))).unsqueeze(0).float()\n",
    "        pred_dinov2_lora = linear_probing(feature_extractor(img_dinov2_lora.to(device))).detach().cpu()\n",
    "\n",
    "        solutions_dinov2_lora['ID'].append(int(test_id))\n",
    "        solutions_dinov2_lora['Pred'].append(int(pred_dinov2_lora.item() > 0.5))\n",
    "        solutions_dinov2_lora_raw['ID'].append(int(test_id))\n",
    "        solutions_dinov2_lora_raw['Raw'].append(pred_dinov2_lora.item())\n",
    "        \n",
    "        # ResNet34 prediction\n",
    "        img_resnet = resnet_preprocessing(torch.tensor(np.array(hdf.get(test_id).get('img')))).unsqueeze(0).float()\n",
    "        pred_resnet = resnet_model(img_resnet.to(device)).detach().cpu()\n",
    "        \n",
    "        solutions_resnet34['ID'].append(int(test_id))\n",
    "        solutions_resnet34['Pred'].append(int(pred_resnet.item() > 0.5))\n",
    "        solutions_resnet34_raw['ID'].append(int(test_id))\n",
    "        solutions_resnet34_raw['Raw'].append(pred_resnet.item())\n",
    "\n",
    "        # Ensemble prediction (average of the two models)\n",
    "        pred_avg = (pred_dinov2_lora.item() + pred_resnet.item()) / 2.0\n",
    "        solutions_dinov2_lora_resnet34['ID'].append(int(test_id))\n",
    "        solutions_dinov2_lora_resnet34['Pred'].append(int(pred_avg > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:19:09.444361Z",
     "iopub.status.busy": "2025-04-08T19:19:09.444046Z",
     "iopub.status.idle": "2025-04-08T19:19:10.151471Z",
     "shell.execute_reply": "2025-04-08T19:19:10.150840Z",
     "shell.execute_reply.started": "2025-04-08T19:19:09.444335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "solutions_dinov2_lora = pd.DataFrame(solutions_dinov2_lora).set_index('ID')\n",
    "solutions_dinov2_lora.to_csv('prediction_dinov2_lora.csv')\n",
    "\n",
    "solutions_dinov2_lora_raw = pd.DataFrame(solutions_dinov2_lora_raw).set_index('ID')\n",
    "solutions_dinov2_lora_raw.to_csv('raw_prediction_dinov2_lora.csv')\n",
    "\n",
    "solutions_resnet34 = pd.DataFrame(solutions_resnet34).set_index('ID')\n",
    "solutions_resnet34.to_csv('prediction_resnet34.csv')\n",
    "\n",
    "solutions_resnet34_raw = pd.DataFrame(solutions_resnet34_raw).set_index('ID')\n",
    "solutions_resnet34_raw.to_csv('raw_prediction_resnet34.csv')\n",
    "\n",
    "solutions_dinov2_lora_resnet34 = pd.DataFrame(solutions_dinov2_lora_resnet34).set_index('ID')\n",
    "solutions_dinov2_lora_resnet34.to_csv('prediction_dinov2_lora_resnet34.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6938476,
     "sourceId": 11125838,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6939142,
     "sourceId": 11126746,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7011798,
     "sourceId": 11226434,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7013786,
     "sourceId": 11229069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7024431,
     "sourceId": 11242762,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7031512,
     "sourceId": 11252143,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7055320,
     "sourceId": 11284343,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "deeplearning_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
